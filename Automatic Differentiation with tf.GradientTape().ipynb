{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797a23a0",
   "metadata": {},
   "source": [
    "# Backpropagation in Tensorflow: The GradientTape context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a7781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95ea17",
   "metadata": {},
   "source": [
    "We first create a set of 20 values randomly sampled between 0 and 100. We then create targets by multiplying the values with pi. \n",
    "\n",
    "So our dataset is sampled from a function $f(x) = \\pi x$\n",
    "\n",
    "Now we want to approximate this function with a linear model that has the form $f_{model}(x) = a x$, where $a$ is the parameter that we want to learn.\n",
    "\n",
    "For this we use the 20 data points, initialize the parameter $a$ to a value far from $\\pi$ and then do gradient descent with the individual examples (also called stochastic gradient descent). We show the entire dataset only once to the model or in other words we only train for one epoch. For gradient descent, we use tensorflow's gradient tape for automatic differentiation to obtain loss gradients with respect to the trainable parameter $a$.\n",
    "\n",
    "\n",
    "What is shown in this notebook regarding the use of tf.GradientTape() can be used for any data and any model. Indeed we can obtain the gradients with respect to millions of parameters with the same general structure, except then we get the list of trainable variables by using \".trainable_variables\" on the tf.keras.Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e86426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple linear univariate model function without bias\n",
    "def model(x, parameter):\n",
    "    return x * parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e355489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 15:13:54.851534: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# generate data (X) and targets (Y)\n",
    "\n",
    "X = tf.random.uniform((20,1), minval= 0, maxval = 10)\n",
    "Y = X * np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534ccae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameter variable to a value far away from pi\n",
    "parameter_estimate = tf.Variable(7.5, trainable=True, dtype=tf.float32)\n",
    "\n",
    "# set learning rate\n",
    "learning_rate = tf.constant(0.005, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df59a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over epochs\n",
    "for epoch in range(2):\n",
    "\n",
    "    # iterate over training examples\n",
    "    for x,y in zip(X,Y):\n",
    "        \n",
    "        # within GradientTape context manager, calculate loss between targets and prediction\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            prediction = model(x, parameter_estimate)\n",
    "\n",
    "            loss = (prediction - y)**2\n",
    "\n",
    "        # outside of context manager, obtain gradients with respect to list of trainable variables\n",
    "        gradients = tape.gradient(loss, [parameter_estimate])\n",
    "\n",
    "        # subtract gradients scaled by learning rate from parameters\n",
    "        new_parameter_val = parameter_estimate - learning_rate * gradients\n",
    "\n",
    "        # assign new parameter values\n",
    "        parameter_estimate.assign(new_parameter_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8e863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14159274\n"
     ]
    }
   ],
   "source": [
    "tf.print(parameter_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f24e5e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(parameter_estimate == np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee542935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "54ff86533a6a943eb33cb0954e5964c6e356fb8134919fff31cf4713965c9c7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
