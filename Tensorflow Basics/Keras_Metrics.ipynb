{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background information: Running neural networks in Tensorflow's Graph Mode vs Eager Mode\n",
    "\n",
    "So far you've probably used numpy and append methods to keep track of your model's performance throughout training and validation. \n",
    "\n",
    "This comes with several disadvantages. One disadvantage is that your training loop (i.e. iterating over the training dataset for a number of epochs, calling the train_step on each batch, and then doing the same for the validation dataset with the test_step) can not run in *graph mode* which is most often more performant compared to the *eager mode* in Tensorflow.\n",
    "\n",
    "Tensorflow was originally built to run deep learning models as a static computational graph that, once constructed, is faster compared to a dynamically built graph. To enable your subclassed model to run and train in graph mode, you need to use the tf.function decorator on the function/method you want to run in graph mode (usually the train_step and test_step). \n",
    "\n",
    "The advantage of using the eager mode instead is that you are allowed to do anything that Python allows you to do. It also gives you more readable error messages for debugging. You can use lists and append to them, you can set attributes of objects etc. What these have in common is that they are considered as **side-effects** of a function - operations and assignments that change global variables inside a function. **Pure functions** on the other hand do not change any variables, they simply return what they compute, which can then be assigned to variables outside of the function. Read more about the specifics here: https://www.tensorflow.org/guide/function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras metrics\n",
    "\n",
    "Instead of appending loss values to a list or a numpy array, we usually rely on something more convenient: tf.keras.metrics objects.\n",
    "\n",
    "Keras comes with useful objects that support tracking a variable, such as the loss over an entire epoch, computing the average over all losses efficiently while allowing for graph mode train and test step functions/methods.\n",
    "\n",
    "\n",
    "Let's assume we have one **epoch** with a small dataset that fills only **4 batches**, so to compute the average loss for this epoch, we want to average over four loss values. \n",
    "\n",
    "We can do this with a tf.keras.metrics.Mean object, which has an **update_state** method, that takes a scalar value to take into account for the running average, a **result** method, to obtain the result, and a **reset_states** method that resets the metric (after an epoch and between the training and validation steps).\n",
    "\n",
    "To see what is going on we print the metric's result after each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss: 17.090959548950195\n",
      "Epoch 0: val_loss: 15.924022674560547 \n",
      "\n",
      "Epoch 1: loss: 16.673707962036133\n",
      "Epoch 1: val_loss: 16.996715545654297 \n",
      "\n",
      "Epoch 2: loss: 16.762340545654297\n",
      "Epoch 2: val_loss: 17.6639347076416 \n",
      "\n",
      "Epoch 3: loss: 17.127212524414062\n",
      "Epoch 3: val_loss: 16.37676429748535 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# instantiate metric object (usually in the model's constructor, i.e. __init__, method)\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "# ITERATING OVER A NUMBER OF EPOCHS:\n",
    "for e in range(4):\n",
    "\n",
    "    # ITERATING OVER TRAINING DATA\n",
    "    for batch in range(1000):\n",
    "\n",
    "        model_output = tf.random.uniform(shape=(4,1))*10\n",
    "        target = tf.random.uniform(shape=(4,1))*10\n",
    "\n",
    "        loss = loss_function(target, model_output)\n",
    "\n",
    "        loss_metric.update_state(values=loss)\n",
    "\n",
    "    tf.print(f\"Epoch {e}: loss: {loss_metric.result()}\")\n",
    "\n",
    "    # RESETTING THE METRICS\n",
    "    loss_metric.reset_states()\n",
    "\n",
    "    # ITERATING OVER VALIDATION DATA\n",
    "    for batch in range(200):\n",
    "\n",
    "        model_output = tf.random.uniform(shape=(4,1))*10\n",
    "\n",
    "        target = tf.random.uniform(shape=(4,1))*10\n",
    "\n",
    "        val_loss = loss_function(target, model_output)\n",
    "\n",
    "        loss_metric.update_state(values=val_loss)\n",
    "\n",
    "    tf.print(f\"Epoch {e}: val_loss: {loss_metric.result()} \\n\")\n",
    "\n",
    "    # RESETTING THE METRIC BEFORE NEXT EPOCH\n",
    "    loss_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy metrics for binary classification\n",
    "As you see we can use tf.keras.metrics.Mean objects to compute running averages without using numpy or list appends. Keras comes with such metric objects for a number of different metrics that we might use to evaluate our model's performance, such as **BinaryAccuracy** in the context of binary classification and **CategoricalAccuracy** **TopKCategoricalAccuracy** in the context of multi-class classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss: 0.9985584020614624\n",
      "Epoch 0: accuracy: 0.5024999976158142\n",
      "Epoch 0: val_loss: 1.083017110824585\n",
      "Epoch 0: val_accuracy: 0.4958333373069763 \n",
      "\n",
      "Epoch 1: loss: 0.9886395335197449\n",
      "Epoch 1: accuracy: 0.4909090995788574\n",
      "Epoch 1: val_loss: 1.0901201963424683\n",
      "Epoch 1: val_accuracy: 0.49270832538604736 \n",
      "\n",
      "Epoch 2: loss: 1.029697299003601\n",
      "Epoch 2: accuracy: 0.4904411733150482\n",
      "Epoch 2: val_loss: 0.9431459307670593\n",
      "Epoch 2: val_accuracy: 0.4902777671813965 \n",
      "\n",
      "Epoch 3: loss: 0.9510965943336487\n",
      "Epoch 3: accuracy: 0.48750001192092896\n",
      "Epoch 3: val_loss: 0.9767802357673645\n",
      "Epoch 3: val_accuracy: 0.4885416626930237 \n",
      "\n",
      "Epoch 4: loss: 0.9949010014533997\n",
      "Epoch 4: accuracy: 0.49698275327682495\n",
      "Epoch 4: val_loss: 1.069822907447815\n",
      "Epoch 4: val_accuracy: 0.49541667103767395 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate metric object (usually in the model's constructor, i.e. __init__, method)\n",
    "loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "binary_accuracy_metric = tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# What happens in one epoch in the training loop:\n",
    "\n",
    "# training on train data (4 batches)\n",
    "\n",
    "for e in range(5):\n",
    "    for train_batch in range(100):\n",
    "        target = tf.random.uniform(shape=(4,1),minval=0, maxval=1, dtype=tf.int32)\n",
    "        model_output = tf.random.uniform(shape=(4,1))\n",
    "\n",
    "        loss = loss_function(target, model_output)\n",
    "        # \n",
    "        loss_metric.update_state(values=loss)\n",
    "        binary_accuracy_metric.update_state(target, model_output)\n",
    "    tf.print(f\"Epoch {e}: loss: {loss_metric.result()}\")\n",
    "    tf.print(f\"Epoch {e}: accuracy: {binary_accuracy_metric.result()}\")\n",
    "\n",
    "    # RESETTING METRICS BEFORE EVALUATION\n",
    "    loss_metric.reset_states()\n",
    "\n",
    "    for val_batch in range(20):\n",
    "        target = tf.random.uniform(shape=(4,1),minval=0, maxval=1, dtype=tf.int32)\n",
    "        model_output = tf.random.uniform(shape=(4,1))\n",
    "\n",
    "        loss = loss_function(target, model_output)\n",
    "        loss_metric.update_state(values=loss)\n",
    "        binary_accuracy_metric.update_state(target, model_output)\n",
    "\n",
    "    tf.print(f\"Epoch {e}: val_loss: {loss_metric.result()}\")\n",
    "    tf.print(f\"Epoch {e}: val_accuracy: {binary_accuracy_metric.result()} \\n\")\n",
    "\n",
    "# resetting the metric before the next epoch\n",
    "loss_metric.reset_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy metrics for multi-class categorization tasks\n",
    "\n",
    "In multi-class classification, we use the **CategoricalCrossentropy** as our loss function. To track the accuracy, we need to use a different keras metric, **CategoricalAccuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss: 2.3434877395629883\n",
      "Epoch 0: accuracy: 0.09749999642372131\n",
      "Epoch 0: val_loss: 2.368985414505005\n",
      "Epoch 0: val_accuracy: 0.1041666641831398 \n",
      "\n",
      "Epoch 1: loss: 2.3544280529022217\n",
      "Epoch 1: accuracy: 0.09772727638483047\n",
      "Epoch 1: val_loss: 2.3128294944763184\n",
      "Epoch 1: val_accuracy: 0.10208333283662796 \n",
      "\n",
      "Epoch 2: loss: 2.337564468383789\n",
      "Epoch 2: accuracy: 0.1044117659330368\n",
      "Epoch 2: val_loss: 2.330190896987915\n",
      "Epoch 2: val_accuracy: 0.10486111044883728 \n",
      "\n",
      "Epoch 3: loss: 2.338282823562622\n",
      "Epoch 3: accuracy: 0.10326086729764938\n",
      "Epoch 3: val_loss: 2.3272953033447266\n",
      "Epoch 3: val_accuracy: 0.10520832985639572 \n",
      "\n",
      "Epoch 4: loss: 2.3113150596618652\n",
      "Epoch 4: accuracy: 0.10517241060733795\n",
      "Epoch 4: val_loss: 2.3447346687316895\n",
      "Epoch 4: val_accuracy: 0.1041666641831398 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate metric object (usually in the model's constructor, i.e. __init__, method)\n",
    "loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "for e in range(5):\n",
    "    for train_batch in range(100):\n",
    "        \n",
    "        # create random one-hot targets (labels)\n",
    "        target = tf.one_hot(tf.random.uniform(minval=0,maxval=9, shape=(4,),dtype=tf.int32), depth=10)\n",
    "\n",
    "        # create random model output (for each element in the batch the values are probabilities that sum to 1)\n",
    "        model_output = tf.nn.softmax(tf.random.uniform(shape=(4,10)), axis=-1)\n",
    "\n",
    "        loss = loss_function(target, model_output)\n",
    "\n",
    "        loss_metric.update_state(values=loss)\n",
    "        accuracy_metric.update_state(target, model_output)\n",
    "    tf.print(f\"Epoch {e}: loss: {loss_metric.result()}\")\n",
    "    tf.print(f\"Epoch {e}: accuracy: {accuracy_metric.result()}\")\n",
    "\n",
    "    # RESETTING METRICS BEFORE EVALUATION\n",
    "    loss_metric.reset_states()\n",
    "\n",
    "    for val_batch in range(20):\n",
    "        target = tf.one_hot(tf.random.uniform(minval=0,maxval=9, shape=(4,),dtype=tf.int32), depth=10)\n",
    "        model_output = tf.nn.softmax(tf.random.uniform(shape=(4,10)), axis=-1)\n",
    "\n",
    "        loss = loss_function(target, model_output)\n",
    "        loss_metric.update_state(values=loss)\n",
    "        accuracy_metric.update_state(target, model_output)\n",
    "\n",
    "    tf.print(f\"Epoch {e}: val_loss: {loss_metric.result()}\")\n",
    "    tf.print(f\"Epoch {e}: val_accuracy: {accuracy_metric.result()} \\n\")\n",
    "\n",
    "    # resetting the metric before the next epoch\n",
    "    loss_metric.reset_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- We can use keras metrics instead of tedious numpy loss tracking and metric computations. \n",
    "\n",
    "- Keras metrics can be used with Tensorflow's graph mode, while list appends and numpy operations generally can not\n",
    "\n",
    "- The convenient compile and fit methods of the tf.keras.Model class use keras metrics under the hood. \n",
    "    - Starting to use the metric objects gets us a step closer to being able to use these tools.\n",
    "\n",
    "\n",
    "\n",
    "Next: **Tensorboard for logging**, log all possible kinds of training data to the tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54ff86533a6a943eb33cb0954e5964c6e356fb8134919fff31cf4713965c9c7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
